{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ccd42f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "def extract_text_from_pdf(file):\n",
    "    with fitz.open(stream=file.read(), filetype=\"pdf\") as doc:\n",
    "        return \" \".join(page.get_text() for page in doc)\n",
    "\n",
    "def clean_text(text, max_chars=3000):\n",
    "    cleaned = \" \".join(text.strip().split())\n",
    "    return cleaned[:max_chars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5767cfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq LLM error: 401 Client Error: Unauthorized for url: https://api.groq.com/openai/v1/chat/completions\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import ast\n",
    "import re\n",
    "import spacy\n",
    "import requests\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# --- PDF Extraction & Cleaning ---\n",
    "def extract_text_from_pdf(file):\n",
    "    with fitz.open(stream=file.read(), filetype=\"pdf\") as doc:\n",
    "        return \" \".join(page.get_text() for page in doc)\n",
    "\n",
    "def clean_text(text):\n",
    "    return \" \".join(text.strip().split())\n",
    "\n",
    "# --- Pre-filter relevant sections ---\n",
    "def extract_relevant_sections(text):\n",
    "    patterns = [\n",
    "        r\"skills[\\s:.-]+(.+?)(?=\\n[A-Z]|\\Z)\",\n",
    "        r\"technical skills[\\s:.-]+(.+?)(?=\\n[A-Z]|\\Z)\",\n",
    "        r\"tools[\\s:.-]+(.+?)(?=\\n[A-Z]|\\Z)\",\n",
    "        r\"technologies[\\s:.-]+(.+?)(?=\\n[A-Z]|\\Z)\",\n",
    "        r\"responsibilities[\\s:.-]+(.+?)(?=\\n[A-Z]|\\Z)\",\n",
    "        r\"roles and responsibilities[\\s:.-]+(.+?)(?=\\n[A-Z]|\\Z)\",\n",
    "        r\"requirements[\\s:.-]+(.+?)(?=\\n[A-Z]|\\Z)\",\n",
    "        r\"job description[\\s:.-]+(.+?)(?=\\n[A-Z]|\\Z)\"\n",
    "    ]\n",
    "    combined_pattern = \"|\".join(patterns)\n",
    "    matches = re.findall(combined_pattern, text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    if matches:\n",
    "        sections = [\" \".join(m).strip() for m in matches if any(m)]\n",
    "        return \" \".join(sections)\n",
    "    return text\n",
    "\n",
    "# --- spaCy candidate filtering ---\n",
    "def spacy_filter_skill_candidates(text):\n",
    "    doc = nlp(text)\n",
    "    candidates = set()\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if len(chunk.text.strip()) > 2 and not chunk.root.is_stop:\n",
    "            candidates.add(chunk.text.strip())\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"PROPN\", \"NOUN\"] and not token.is_stop and len(token.text.strip()) > 2:\n",
    "            candidates.add(token.text.strip())\n",
    "    return \" \".join(candidates)\n",
    "\n",
    "# --- Groq LLM Skill Extractor ---\n",
    "def extract_skills_resume_and_jd_groq(\n",
    "    resume_file,\n",
    "    jd_file=None,\n",
    "    jd_text=None,\n",
    "    groq_model=\"llama3-8b-8192\",  # ✅ use a valid Groq model\n",
    "    api_key=\"YOUR_GROQ_API_KEY\"\n",
    "):\n",
    "    # --- Extract Resume ---\n",
    "    resume_text = clean_text(extract_text_from_pdf(resume_file))\n",
    "    resume_filtered = extract_relevant_sections(resume_text)\n",
    "    resume_candidates = spacy_filter_skill_candidates(resume_filtered)\n",
    "\n",
    "    # --- Extract Job Description ---\n",
    "    if jd_file:\n",
    "        jd_text_content = clean_text(extract_text_from_pdf(jd_file))\n",
    "    else:\n",
    "        jd_text_content = clean_text(jd_text or \"\")\n",
    "    jd_filtered = extract_relevant_sections(jd_text_content)\n",
    "    jd_candidates = spacy_filter_skill_candidates(jd_filtered)\n",
    "\n",
    "import requests\n",
    "import ast\n",
    "\n",
    "api_key = \"gsk_GMCcejfaLSrdkPkc3yxzWGltadSG1hwQrahQYbj7uovqv9\"\n",
    "groq_model = \"llama3-8b-8192\"  # valid Groq model\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are an AI skill extractor.\n",
    "Extract ONLY technical skills (programming languages, frameworks, libraries, tools, platforms) \n",
    "separately for Resume and Job Description.\n",
    "Return JSON:\n",
    "{\"resume_skills\": [...], \"jd_skills\": [...]}\n",
    "\n",
    "Resume:\n",
    "Python, SQL, Tableau\n",
    "\n",
    "Job Description:\n",
    "Python, Spark, AWS\n",
    "\"\"\"\n",
    "\n",
    "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "payload = {\n",
    "    \"model\": groq_model,\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "    \"temperature\": 0\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    llm_output = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    skills_dict = ast.literal_eval(llm_output.strip())\n",
    "    print(skills_dict)\n",
    "except Exception as e:\n",
    "    print(f\"Groq LLM error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c69d5e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "with open(\"sneha_sathe.pdf\", \"rb\") as resume_file:\n",
    "    skills = extract_skills_resume_and_jd_groq(\n",
    "        resume_file,  # PDF file object\n",
    "        jd_text=\"\"\"\n",
    "Requirements\n",
    "• Currently enrolled in or have recently graduated from a degree program in computer science, data science, or a related field\n",
    "• Familiarity with AI and machine learning concepts and techniques\n",
    "• Experience with programming languages such as Python, R and SQL\n",
    "• Strong analytical and problem-solving skills\n",
    "• Strong communication and teamwork skills\n",
    "• Experience with machine learning libraries such as scikit-learn, TensorFlow, and/or Keras is a plus.\n",
    "\"\"\"\n",
    "    )\n",
    "print(skills)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b3cdabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'resume_skills': ['Python', 'Django', 'SQLite', 'Wordpress', 'HTML5', 'CSS3', 'MySQL', 'Machine Learning', 'Statistics'], 'jd_skills': ['Python', 'TensorFlow', 'scikit', 'Keras', 'R', 'SQL', 'AWS', 'Spark']}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import spacy\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Load spaCy (optional, can skip for speed)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# --- PDF Extraction ---\n",
    "def extract_text_from_pdf(file):\n",
    "    with fitz.open(stream=file.read(), filetype=\"pdf\") as doc:\n",
    "        return \" \".join(page.get_text() for page in doc)\n",
    "\n",
    "def clean_text(text):\n",
    "    return \" \".join(text.strip().split())\n",
    "\n",
    "# --- Section Filtering ---\n",
    "def extract_relevant_sections(text):\n",
    "    patterns = [\n",
    "        r\"skills[\\s:.-]+(.+?)(?=\\n[A-Z]|\\Z)\",\n",
    "        r\"technical skills[\\s:.-]+(.+?)(?=\\n[A-Z]|\\Z)\",\n",
    "        r\"tools[\\s:.-]+(.+?)(?=\\n[A-Z]|\\Z)\",\n",
    "        r\"technologies[\\s:.-]+(.+?)(?=\\n[A-Z]|\\Z)\",\n",
    "        r\"responsibilities[\\s:.-]+(.+?)(?=\\n[A-Z]|\\Z)\",\n",
    "        r\"roles and responsibilities[\\s:.-]+(.+?)(?=\\n[A-Z]|\\Z)\",\n",
    "        r\"requirements[\\s:.-]+(.+?)(?=\\n[A-Z]|\\Z)\",\n",
    "        r\"job description[\\s:.-]+(.+?)(?=\\n[A-Z]|\\Z)\"\n",
    "    ]\n",
    "    matches = re.findall(\"|\".join(patterns), text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    if matches:\n",
    "        sections = [\" \".join(m).strip() for m in matches if any(m)]\n",
    "        return \" \".join(sections)\n",
    "    return text\n",
    "\n",
    "# --- spaCy Filtering ---\n",
    "def spacy_filter_skill_candidates(text):\n",
    "    doc = nlp(text)\n",
    "    candidates = set()\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if len(chunk.text.strip()) > 2 and not chunk.root.is_stop:\n",
    "            candidates.add(chunk.text.strip())\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"PROPN\", \"NOUN\"] and not token.is_stop:\n",
    "            candidates.add(token.text.strip())\n",
    "    return \", \".join(candidates)\n",
    "\n",
    "# --- Main Function ---\n",
    "def extract_skills_resume_and_jd_groq(\n",
    "    resume_file,\n",
    "    jd_file=None,\n",
    "    jd_text=None,\n",
    "    groq_model=\"llama3-8b-8192\"\n",
    "):\n",
    "    # API Key from environment (run: export GROQ_API_KEY=\"your_key_here\")\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    if not api_key:\n",
    "        return {\"error\": \"Missing Groq API key. Set GROQ_API_KEY environment variable.\"}\n",
    "\n",
    "    # --- Resume Processing ---\n",
    "    resume_text = clean_text(extract_text_from_pdf(resume_file))\n",
    "    resume_filtered = extract_relevant_sections(resume_text)\n",
    "    resume_candidates = spacy_filter_skill_candidates(resume_filtered)\n",
    "\n",
    "    # --- JD Processing ---\n",
    "    if jd_file:\n",
    "        jd_text_content = clean_text(extract_text_from_pdf(jd_file))\n",
    "    else:\n",
    "        jd_text_content = clean_text(jd_text or \"\")\n",
    "    jd_filtered = extract_relevant_sections(jd_text_content)\n",
    "    jd_candidates = spacy_filter_skill_candidates(jd_filtered)\n",
    "\n",
    "    # --- LLM Prompt ---\n",
    "    prompt = f\"\"\"\n",
    "    Extract ONLY technical skills (programming languages, frameworks, libraries, tools, platforms) \n",
    "    separately for Resume and Job Description.\n",
    "    Respond ONLY with valid JSON, no markdown, no extra text.\n",
    "    Format:\n",
    "    {{\n",
    "      \"resume_skills\": [...],\n",
    "      \"jd_skills\": [...]\n",
    "    }}\n",
    "\n",
    "    Resume:\n",
    "    {resume_candidates}\n",
    "\n",
    "    Job Description:\n",
    "    {jd_candidates}\n",
    "    \"\"\"\n",
    "\n",
    "    # --- API Request ---\n",
    "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": groq_model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        llm_output = response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "        # Remove code fences if any\n",
    "        if llm_output.startswith(\"```\"):\n",
    "            llm_output = llm_output.split(\"```\")[1]\n",
    "            if llm_output.startswith(\"json\"):\n",
    "                llm_output = llm_output[len(\"json\"):].strip()\n",
    "\n",
    "        # Parse JSON\n",
    "        return json.loads(llm_output)\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\"error\": f\"JSON parsing error: {e}\", \"raw_output\": llm_output}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"sneha_sathe.pdf\", \"rb\") as resume_file:\n",
    "        skills = extract_skills_resume_and_jd_groq(\n",
    "            resume_file,\n",
    "            jd_text=\"\"\"\n",
    "            Requirements\n",
    "            • Experience with Python, R, SQL\n",
    "            • Knowledge of TensorFlow, scikit-learn, Keras\n",
    "            • Familiarity with AWS, Spark\n",
    "            \"\"\"\n",
    "        )\n",
    "    print(skills)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f0bf8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "def is_ollama_model_installed(model_name):\n",
    "    \"\"\"Check if the Ollama model is installed locally.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"list\"], capture_output=True, text=True\n",
    "        )\n",
    "        return model_name in result.stdout\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def extract_text_from_pdf(file):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    pdf_document = fitz.open(stream=file.read(), filetype=\"pdf\")\n",
    "    text = \"\"\n",
    "    for page in pdf_document:\n",
    "        text += page.get_text()\n",
    "    return text.strip()\n",
    "\n",
    "def extract_skills_resume_and_jd(resume_file=None, jd_file=None, resume_text=None, jd_text=None, model=\"mistral\"):\n",
    "    \"\"\"Extract skills from Resume and Job Description using Ollama.\"\"\"\n",
    "  \n",
    "    # Get resume text\n",
    "    if resume_file:\n",
    "        resume_text_content = extract_text_from_pdf(resume_file)\n",
    "    else:\n",
    "        resume_text_content = resume_text or \"\"\n",
    "    \n",
    "    # Get JD text\n",
    "    if jd_file:\n",
    "        jd_text_content = extract_text_from_pdf(jd_file)\n",
    "    else:\n",
    "        jd_text_content = jd_text or \"\"\n",
    "\n",
    "    # Prepare prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are a technical skill extraction assistant.\n",
    "    Extract ONLY technical skills (programming languages, frameworks, tools, libraries, cloud platforms, etc.)\n",
    "    from the following Resume and Job Description.\n",
    "    Provide the output as JSON with keys 'resume_skills' and 'jd_skills'.\n",
    "\n",
    "    Resume:\n",
    "    {resume_text_content}\n",
    "\n",
    "    Job Description:\n",
    "    {jd_text_content}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", model, prompt],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        output_text = result.stdout.strip()\n",
    "        try:\n",
    "            skills = json.loads(output_text)\n",
    "        except json.JSONDecodeError:\n",
    "            skills = {\"resume_skills\": [], \"jd_skills\": []}\n",
    "        return skills\n",
    "    except Exception as e:\n",
    "        return {\"resume_skills\": [], \"jd_skills\": [], \"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13be6cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'resume_skills': ['Python', 'Django', 'SQLite', 'Wordpress', 'HTML5', 'CSS3', 'MySQL', 'Machine Learning', 'Statistics'], 'jd_skills': ['Python', 'TensorFlow', 'scikit', 'Keras', 'R', 'SQL', 'AWS', 'Spark']}\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"sneha_sathe.pdf\", \"rb\") as resume_file:\n",
    "        skills = extract_skills_resume_and_jd_groq(\n",
    "            resume_file,\n",
    "            jd_text=\"\"\"\n",
    "            Requirements\n",
    "            • Experience with Python, R, SQL\n",
    "            • Knowledge of TensorFlow, scikit-learn, Keras\n",
    "            • Familiarity with AWS, Spark\n",
    "            \"\"\"\n",
    "        )\n",
    "    print(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282588c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
